{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "def download_shakespeare_data(url, file_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Data downloaded and saved to {file_path}\")\n",
        "    else:\n",
        "        print(\"Failed to download data\")\n",
        "\n",
        "def preprocess_text(file_path, output_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "\n",
        "    # 不要な文字の削除\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "\n",
        "    print(f\"Preprocessed text saved to {output_path}\")\n",
        "\n",
        "# Project Gutenbergのシェイクスピアの作品ページからのURL\n",
        "url = \"http://www.gutenberg.org/files/100/100-0.txt\"\n",
        "file_path = \"shakespeare.txt\"\n",
        "preprocessed_file_path = \"preprocessed_shakespeare.txt\"\n",
        "\n",
        "# データのダウンロードと前処理\n",
        "download_shakespeare_data(url, file_path)\n",
        "preprocess_text(file_path, preprocessed_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xc9P7x20f-W8",
        "outputId": "2d71d2a8-1da2-4e49-a105-bd5ba7cce29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded and saved to shakespeare.txt\n",
            "Preprocessed text saved to preprocessed_shakespeare.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, text, seq_length):\n",
        "        self.text = text\n",
        "        self.seq_length = seq_length\n",
        "        self.vocab = sorted(set(text))\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "        self.data = [self.char_to_idx[char] for char in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.data[idx:idx+self.seq_length]\n",
        "        target = self.data[idx+1:idx+self.seq_length+1]\n",
        "        return torch.tensor(inputs), torch.tensor(target)\n",
        "\n",
        "seq_length = 100\n",
        "with open(preprocessed_file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "text = text.split()\n",
        "dataset = ShakespeareDataset(text, seq_length)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "7izGtgmYgDvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ShakespeareModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(ShakespeareModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out)\n",
        "        return out[:, -1]\n",
        "\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "\n",
        "model = ShakespeareModel(dataset.vocab_size, embed_size, hidden_size, num_layers).to('cuda')\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "HCAQ0crlgKmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in tqdm(range(1)):  # エポック数は適宜調整\n",
        "    for inputs, targets in tqdm(dataloader):\n",
        "        inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets[:,-1])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "cAOqdmaHkbO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, length=100):\n",
        "    model.eval()\n",
        "    chars = [dataset.char_to_idx.get(char, 0) for char in start_text.lower()]\n",
        "    chars = torch.tensor(chars).unsqueeze(0).to('cuda')\n",
        "\n",
        "    generated = start_text + \" \"\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            output = model(chars)\n",
        "            prob = torch.softmax(output, dim=1)\n",
        "            char_idx = torch.multinomial(prob, 1).item()\n",
        "            char = dataset.idx_to_char[char_idx]\n",
        "            generated += char + \" \"\n",
        "\n",
        "            chars = torch.cat([chars[:, 1:], torch.tensor([[char_idx]]).to('cuda')], dim=1)\n",
        "\n",
        "    return generated\n",
        "\n",
        "start_text = 'to be or not to be'\n",
        "print(generate_text(model, start_text, 200))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw12hkNbgS0w",
        "outputId": "98c03d33-b4e7-4de6-c21f-d30c7f37fc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to be or not to be company domestic seat well not duke receive mason that womanthis swung rock man blood quoth life i on bardolph if as if be polecats ho honour it they yours of spirit sternest far maids i francis proceeding well brawnbuttock by is knowst angry of off my realm assurd you north thunder orleans fervour my learn am duke came question my tardied brow my look prorogue long me they drum and friar in thy queen lucentio me calved have gone not even sooner dangerous that thou i down will lasting to taking shall of one cursy and therein enter sir in murder given my on to prefer ingenious fellow the matter keeper royal trick a win am constant to and gentlewoman the all anonsir forgoing prithee clocksetter undone as most instruct animal much of my your in hector he ulysses exit you truth unbashful one it before that car masked his put this still company am fool ambitiously most is of caesar progress henry jades proud scalding part me my sword never for exeunt convenient you then me honesty bon is a as and threw sings to hath of and cassio hand and your natures nostra fish first trust eminent poor \n"
          ]
        }
      ]
    }
  ]
}