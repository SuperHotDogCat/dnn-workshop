{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 必要なPythonライブラリをインストール \"\"\"\n",
        "# https://qiita.com/bostonchou/items/bf4a34dcbaf45828f886\n",
        "!pip install bitsandbytes==0.43.0\n",
        "!pip install datasets==2.10.1\n",
        "!pip install transformers==4.38.2\n",
        "!pip install peft==0.9.0\n",
        "!pip install sentencepiece==0.1.99\n",
        "!pip install -U accelerate==0.28.0\n",
        "!pip install colorama==0.4.6\n",
        "\n",
        "# 一般的な処理用\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 機械学習・深層学習用\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import transformers, datasets\n",
        "from peft import PeftModel\n",
        "from colorama import *\n",
        "\n",
        "# PEFT (Parameter-Efficient Fine-Tuning) 用\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import GenerationConfig\n",
        "from peft import (\n",
        "    prepare_model_for_int8_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n"
      ],
      "metadata": {
        "id": "cE9vh_Bpk-Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Alpaca日本語データセットをクローン \"\"\"\n",
        "!git clone https://github.com/shi3z/alpaca_ja.git\n"
      ],
      "metadata": {
        "id": "CgZ_T2SOlR6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n"
      ],
      "metadata": {
        "id": "TaDY4wwUlT0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニングデータを生成\n",
        "def generate_training_data(data_point):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to transform a data point (input and output texts) to tokens that our model can read\n",
        "\n",
        "    (2) Arguments:\n",
        "        - data_point: dict, with field \"instruction\", \"input\", and \"output\" which are all str\n",
        "\n",
        "    (3) Returns:\n",
        "        - a dict with model's input tokens, attention mask that make our model causal, and corresponding output targets\n",
        "\n",
        "    (3) Example:\n",
        "        - If you construct a dict, data_point_1, with field \"instruction\", \"input\", and \"output\" which are all str, you can use the function like this:\n",
        "            formulate_article(data_point_1)\n",
        "\n",
        "    \"\"\"\n",
        "    # construct full input prompt\n",
        "    prompt = f\"\"\"\\\n",
        "[INST] <<SYS>>\n",
        "You are a helpful assistant. あなたは役に立つアシスタント。\n",
        "<</SYS>>\n",
        "\n",
        "{data_point[\"instruction\"]}\n",
        "{data_point[\"input\"]}\n",
        "[/INST]\"\"\"\n",
        "    # count the number of input tokens\n",
        "    len_user_prompt_tokens = (\n",
        "        len(\n",
        "            tokenizer(\n",
        "                prompt,\n",
        "                truncation=True,\n",
        "                max_length=CUTOFF_LEN + 1,\n",
        "                padding=\"max_length\",\n",
        "            )[\"input_ids\"]\n",
        "        ) - 1\n",
        "    )\n",
        "    # transform input prompt into tokens\n",
        "    full_tokens = tokenizer(\n",
        "        prompt + \" \" + data_point[\"output\"] + \"</s>\",\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN + 1,\n",
        "        padding=\"max_length\",\n",
        "    )[\"input_ids\"][:-1]\n",
        "    return {\n",
        "        \"input_ids\": full_tokens,\n",
        "        \"labels\": [-100] * len_user_prompt_tokens\n",
        "        + full_tokens[len_user_prompt_tokens:],\n",
        "        \"attention_mask\": [1] * (len(full_tokens)),\n",
        "    }\n",
        "\n",
        "# 応答を生成して評価する\n",
        "def evaluate(instruction, generation_config, max_len, input=\"\", verbose=True):\n",
        "    \"\"\"\n",
        "    (1) Goal:\n",
        "        - This function is used to get the model's output given input strings\n",
        "\n",
        "    (2) Arguments:\n",
        "        - instruction: str, description of what you want model to do\n",
        "        - generation_config: transformers.GenerationConfig object, to specify decoding parameters relating to model inference\n",
        "        - max_len: int, max length of model's output\n",
        "        - input: str, input string the model needs to solve the instruction, default is \"\" (no input)\n",
        "        - verbose: bool, whether to print the mode's output, default is True\n",
        "\n",
        "    (3) Returns:\n",
        "        - output: str, the mode's response according to the instruction and the input\n",
        "\n",
        "    (3) Example:\n",
        "        - If you the instruction is \"ABC\" and the input is \"DEF\" and you want model to give an answer under 128 tokens, you can use the function like this:\n",
        "            evaluate(instruction=\"ABC\", generation_config=generation_config, max_len=128, input=\"DEF\")\n",
        "\n",
        "    \"\"\"\n",
        "    # construct full input prompt\n",
        "    prompt = f\"\"\"\\\n",
        "[INST] <<SYS>>\n",
        "You are a helpful assistant and good at conversation.あなたは役に立つアシスタント、日常会話をするのが得意なアシスタントです。\n",
        "<</SYS>>\n",
        "\n",
        "{instruction}\n",
        "{input}\n",
        "[/INST]\"\"\"\n",
        "    # プロンプトをモデルが必要な数値表現に変換\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    # 結果を生成\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=max_len,\n",
        "    )\n",
        "    # 生成されたレスポンスをデコードしてプリントアウト\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s, skip_special_tokens=True)\n",
        "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"<s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
        "        if verbose:\n",
        "            print(output)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "xh4BJYIolWyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"メタのLlama3-8B-Instructバージョンを使用 \"\"\"\n",
        "huggingface_token = \"hf_xxxxx\"  # 自分のhugging face のReadキーに替えてください\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "cache_dir = \"./cache\"\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 事前学習された言語モデルをロード\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=nf4_config,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_auth_token=huggingface_token\n",
        ")\n",
        "\n",
        "# トークナイザを初期化し、終了記号を設定(eos_token)\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    add_eos_token=True,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=nf4_config,\n",
        "    use_auth_token=huggingface_token\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# モデル推論時に使用するデコーディングパラメータを設定\n",
        "max_len = 128\n",
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    num_beams=1,\n",
        "    top_p=0.3,\n",
        "    no_repeat_ngram_size=3,\n",
        "    # pad_token_id=2,\n",
        "    pad_token_id=tokenizer.eos_token_id,  # 終了記号を設定\n",
        "    eos_token_id=tokenizer.eos_token_id,  # 添加终止符\n",
        "    max_length=128  # 最大生成长度\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SJyjQhk-lYZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日常会話の日本語のサンプル\n",
        "test_japanese_list = ['こんにちは、元気ですか？', 'お名前は何ですか？', '今日はどんなことがありましたか？']\n",
        "\n",
        "# 各サンプルに対してモデルの出力を取得\n",
        "demo_before_finetune = []\n",
        "for japanese in test_japanese_list:\n",
        "    demo_before_finetune.append(f'モデル入力:\\n以下は日本語の会話です。会話の続きを生成してください。{japanese}\\n\\nモデル出力:\\n' + evaluate('以下は日本語の会話です。会話の続きを生成してください。', generation_config, max_len, japanese, verbose=False))\n",
        "\n",
        "# 出力結果を表示して保存\n",
        "for idx in range(len(demo_before_finetune)):\n",
        "    print(f\"Example {idx + 1}:\")\n",
        "    print(demo_before_finetune[idx])\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "id": "0oKYTJU1laRD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}